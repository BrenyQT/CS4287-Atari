{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BrenyQT/CS4287-Atari/blob/main/CS4287_Prj2_21315205_ID2_ID3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Brendan Quinn: 21315205\n",
        "Kevin Hough:\n",
        "Sophie Quinn:\n",
        "\n",
        "2. The code excecutes without error\n",
        "\n",
        "3. Resources:"
      ],
      "metadata": {
        "id": "QjJeyuzA2qCh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. **Why Reinforcement Learning is the machine learning paradigm of choice for this task:**\n",
        "\n",
        "Reinforcement Learning is suitable for solving Atari Pong because:\n",
        "\n",
        " **Sequential Decision Making:** Pong involves moving a paddle up and down to hit aball back toward an opponent. This means a decision needs to be made at each time step. The agents goal is to maximise the games score by leatrning an optimal sequence of movements. Reinforcement Learning allows for solving problems where the current decision impacts future rewards.\n",
        "\n",
        "**Dynamic and Interactive Learning Environment:** Pong is an interactive game where the balls position and direction are directly influenced bt the agents actions and the opponents behaviour. Reinforcemeent Learning adapts to the eveolving enviroment by learning through interaction.\n",
        "\n",
        "**Reward Driven Learning:** The reward signal in pong is straightforward. A postive reward is given for scoring a point. A negative reward is given for coding a point. Reinforcement Learning is designed to optimize cumulative rewards. This makes it well suited for tasks with such clearly defined goals."
      ],
      "metadata": {
        "id": "yraE5Gjq-2oH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. **The Gym Environment:**\n",
        " The Gym Enviroment was developled by Open AI in 2016. The Gym Enviroment is a widely used API for developing and eveluating Reinforcement Learning algorithms. It provides a standerdized interface and a diverse set of enviroments, including classic control tasks, board games and Atari games. This standeardization provides a benchmarking enviroment where several Reinforcemnt Learning algorithms can be tested.\n",
        "\n",
        "**Standerdized Interface:** Gym provides an API for interacting with various enviroments. This median of stardardization allows researchers to apply their Reinforcement Learning algorithms accross different tasks without significant change.\n",
        "\n",
        "**Predefined Enviroments:** Gym includes a collection of predefined enviroments such as **'Pong-v0'**. These enviroments come with a predefined state and action spaces, reward structures and dynamics. This allows researchers to focus on algorithm developments rather than enviroment creation.\n",
        "\n",
        "**Visualization Tools:** Gym provides rendering capbilities to visualize the agents interaction with the enviroment. This aid the debugging process.\n",
        "\n",
        "By utilizing the Gym Enviroment we can leverage  a standardized platform to develop, test and benchmark our Reinforcement Learing Algorithm effectively.\n"
      ],
      "metadata": {
        "id": "W4u3HumH-5Ww"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. **Implementation:**\n"
      ],
      "metadata": {
        "id": "XLs3eqtO-_O9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**A. Capture and pre-processing of the data (3 marks).**"
      ],
      "metadata": {
        "id": "5doi9t11dyde"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IFiGalb0enKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**B. The network structure (4 marks).**"
      ],
      "metadata": {
        "id": "O-BMCK0Zd6kO"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hb2o7Honenyv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**C. A very clear and extremely detailed discussion on the code where the Q learning update applied to the weights (3 marks).**\n",
        "\n",
        "For example: what is the update, how is the error\n",
        "calculated?<br><br>"
      ],
      "metadata": {
        "id": "R3eQsUPreHQG"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qbq_0aBVeoR-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**D. Independently researched concepts such as random seed initialisation, the impact of regularisers on scores, and techniques to counter catastrophic forgetting and maximisation bias (5 marks).**\n",
        "\n",
        "An attempt must be made to code and evaluate the efficacy of one of these\n",
        "concepts. Coding fragments and/or diagrams should be included to illustrate the concepts under\n",
        "discussion\n",
        "\n",
        "An attempt must be made to code and evaluate the efficacy of one of these\n",
        "concepts. Coding fragments and/or diagrams should be included to illustrate the concepts under discussion."
      ],
      "metadata": {
        "id": "-608olUfeXv2"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OFY6sdiTeowW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. **Plots:**"
      ],
      "metadata": {
        "id": "oDfaSyiJfRHH"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "99yYBdcqfeZm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. **Videos and plots with short accompanying explanations of the information conveyed:**"
      ],
      "metadata": {
        "id": "eLk1nE7_fezn"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g3uKOMKhfkzG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. **Evaluation of the results:**\n",
        "\n",
        "A. How does one evaluate the performance of the RL agent?\n",
        "\n",
        "B. Are the metrics that we have seen to date relevant?"
      ],
      "metadata": {
        "id": "FOBHy1g3fn5f"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eGdygvG6fsU2"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. **References:**"
      ],
      "metadata": {
        "id": "4oXDj-PUf_SX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries\n",
        "!pip install tensorflow opencv-python matplotlib numpy\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import models, layers, optimizers\n",
        "import random\n",
        "from collections import deque\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "metadata": {
        "id": "cje1nr9g_BIh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90cfd9fc-70c7-468b-f82c-7d0650d9c79e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.25.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.68.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.1)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.5.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.55.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing function\n",
        "def preprocess_frame(frame):\n",
        "    # Convert to grayscale and resize to (84, 84)]\n",
        "    if frame.ndim == 3 and frame.shape[2] == 3:\n",
        "      frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
        "    frame = cv2.resize(frame, (84, 84))\n",
        "    return frame / 255.0  # Normalize pixel values\n",
        "\n",
        "# Stack frames for temporal information\n",
        "def stack_frames(stack, new_frame, stack_size=4):\n",
        "    if stack is None:\n",
        "        # Initialize stack with repeated frames\n",
        "        stack = [new_frame for _ in range(stack_size)]\n",
        "    else:\n",
        "        # Remove oldest frame and add new frame\n",
        "        stack = list(stack)\n",
        "        stack.pop(0)\n",
        "        stack.append(new_frame)\n",
        "    return np.stack(stack, axis=-1)\n"
      ],
      "metadata": {
        "id": "Oa8O5AQT_fNP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_dqn(input_shape, action_space):\n",
        "    model = models.Sequential([\n",
        "        layers.Conv2D(32, (8, 8), strides=4, activation='relu', input_shape=input_shape),\n",
        "        layers.Conv2D(64, (4, 4), strides=2, activation='relu'),\n",
        "        layers.Conv2D(64, (3, 3), strides=1, activation='relu'),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dense(action_space, activation='linear')\n",
        "    ])\n",
        "    model.compile(optimizer=optimizers.Adam(learning_rate=0.0001), loss='mse')\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "zRla8ZGj_kWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DQNAgent:\n",
        "    def __init__(self, action_space):\n",
        "        self.action_space = action_space\n",
        "        self.memory = deque(maxlen=20000)\n",
        "        self.gamma = 0.99  # Discount factor\n",
        "        self.epsilon = 1.0  # Exploration rate\n",
        "        self.epsilon_min = 0.1\n",
        "        self.epsilon_decay = 0.995\n",
        "        self.batch_size = 64\n",
        "        self.model = build_dqn((84, 84, 4), action_space)\n",
        "        self.target_model = build_dqn((84, 84, 4), action_space)\n",
        "        self.update_target_model()\n",
        "\n",
        "    def update_target_model(self):\n",
        "        self.target_model.set_weights(self.model.get_weights())\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def act(self, state):\n",
        "        if np.random.rand() < self.epsilon:\n",
        "            return random.randrange(self.action_space)\n",
        "        q_values = self.model.predict(state, verbose=0)\n",
        "        return np.argmax(q_values[0])\n",
        "\n",
        "    def replay(self):\n",
        "        if len(self.memory) < self.batch_size:\n",
        "            return\n",
        "\n",
        "        batch = random.sample(self.memory, self.batch_size)\n",
        "        for state, action, reward, next_state, done in batch:\n",
        "            target = self.model.predict(state, verbose=0)\n",
        "            if done:\n",
        "                target[0][action] = reward\n",
        "            else:\n",
        "                t = self.target_model.predict(next_state, verbose=0)\n",
        "                target[0][action] = reward + self.gamma * np.amax(t[0])\n",
        "\n",
        "            self.model.fit(state, target, epochs=1, verbose=0)\n",
        "\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n"
      ],
      "metadata": {
        "id": "3jkWs9QU_lhC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gym[atari] autorom[accept-rom-license]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wcxJKAisorwY",
        "outputId": "8dbf427e-70b4-4cef-b8ef-c5305aa5e944"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym[atari] in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Collecting autorom[accept-rom-license]\n",
            "  Downloading AutoROM-0.6.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym[atari]) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym[atari]) (3.1.0)\n",
            "Requirement already satisfied: gym_notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym[atari]) (0.0.8)\n",
            "Requirement already satisfied: ale-py~=0.7.5 in /usr/local/lib/python3.10/dist-packages (from gym[atari]) (0.7.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]) (8.1.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]) (2.32.3)\n",
            "Collecting AutoROM.accept-rom-license (from autorom[accept-rom-license])\n",
            "  Downloading AutoROM.accept-rom-license-0.6.1.tar.gz (434 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.7/434.7 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.7.5->gym[atari]) (6.4.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]) (2024.8.30)\n",
            "Downloading AutoROM-0.6.1-py3-none-any.whl (9.4 kB)\n",
            "Building wheels for collected packages: AutoROM.accept-rom-license\n",
            "  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.6.1-py3-none-any.whl size=446667 sha256=7d99645f021e86937d22f9997365e326fe8dd3707a8340d12dff3f2fb9582f30\n",
            "  Stored in directory: /root/.cache/pip/wheels/6b/1b/ef/a43ff1a2f1736d5711faa1ba4c1f61be1131b8899e6a057811\n",
            "Successfully built AutoROM.accept-rom-license\n",
            "Installing collected packages: AutoROM.accept-rom-license, autorom\n",
            "Successfully installed AutoROM.accept-rom-license-0.6.1 autorom-0.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium[atari] autorom[accept-rom-licesnse]\n",
        "!AutoROM --accept-license\n",
        "!pip install ale-py\n",
        "\n",
        "import ale_py\n",
        "import gymnasium as gym\n",
        "\n",
        "gym.register_envs(ale_py)\n",
        "\n",
        "env = gym.make('ALE/Pong-v5')\n",
        "\n",
        "agent = DQNAgent(action_space=env.action_space.n)\n",
        "episodes = 500\n",
        "stack_size = 4\n",
        "\n",
        "rewards = []  # Initialize rewards list\n",
        "\n",
        "for episode in range(episodes):\n",
        "    state, info = env.reset()\n",
        "    state = preprocess_frame(state)  # Use the initial state returned by env.reset()\n",
        "    state_stack = stack_frames(None, state, stack_size)\n",
        "    state_stack = np.expand_dims(state_stack, axis=0)\n",
        "\n",
        "    total_reward = 0\n",
        "    done = False\n",
        "    while not done:\n",
        "        action = agent.act(state_stack)\n",
        "        next_state, reward, done, truncated, info = env.step(action)\n",
        "        total_reward += reward\n",
        "\n",
        "        if done or truncated:\n",
        "            break\n",
        "\n",
        "        next_state = preprocess_frame(next_state)\n",
        "        next_state_stack = stack_frames(state_stack[0], next_state, stack_size)\n",
        "        next_state_stack = np.expand_dims(next_state_stack, axis=0)\n",
        "\n",
        "        agent.remember(state_stack, action, reward, next_state_stack, done)\n",
        "        state_stack = next_state_stack\n",
        "\n",
        "        agent.replay()\n",
        "\n",
        "    agent.update_target_model()\n",
        "    print(f\"Episode: {episode + 1}, Total Reward: {total_reward}, Epsilon: {agent.epsilon:.2f}\")\n",
        "    rewards.append(total_reward)  # Append reward to list\n",
        "\n",
        "# Plot rewards over episodes\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(rewards)\n",
        "plt.xlabel('Episodes')\n",
        "plt.ylabel('Total Reward')\n",
        "plt.title('Performance of DQN on Pong')\n",
        "plt.show()\n",
        "\n",
        "# Save model\n",
        "agent.model.save('pong_dqn.h5')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "720zYmJv_oRg",
        "outputId": "829bdcd9-42dc-4edf-d698-5a7a78614c86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium[atari] in /usr/local/lib/python3.10/dist-packages (1.0.0)\n",
            "Requirement already satisfied: autorom[accept-rom-licesnse] in /usr/local/lib/python3.10/dist-packages (0.6.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (0.0.4)\n",
            "Requirement already satisfied: ale-py>=0.9 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (0.10.1)\n",
            "\u001b[33mWARNING: autorom 0.6.1 does not provide the extra 'accept-rom-licesnse'\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-licesnse]) (8.1.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-licesnse]) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-licesnse]) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-licesnse]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-licesnse]) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-licesnse]) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot rewards over episodes\n",
        "plt.plot(rewards)\n",
        "plt.xlabel('Episodes')\n",
        "plt.ylabel('Total Reward')\n",
        "plt.title('Performance of DQN on Pong')\n",
        "plt.show()\n",
        "\n",
        "# Save model\n",
        "agent.model.save('pong_dqn.h5')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "HAk3l7YX_pw1",
        "outputId": "756f3cec-a79f-47eb-92d1-55c758cc3062"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'rewards' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-7c971035075e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Plot rewards over episodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Episodes'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Total Reward'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Performance of DQN on Pong'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'rewards' is not defined"
          ]
        }
      ]
    }
  ]
}